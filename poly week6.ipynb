{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3183c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from random import seed\n",
    "from random import randrange\n",
    "# importing StandardScaler library        \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fe1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression:\n",
    "    def __init__(self, degree=2, lr=0.001, n_iters=1000):\n",
    "        self.degree = degree\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Add polynomial features\n",
    "        X_poly = self._polynomial_features(X, self.degree)\n",
    "\n",
    "        # Normalize data\n",
    "        self.scaler = StandardScaler()\n",
    "        X_poly = self.scaler.fit_transform(X_poly)\n",
    "\n",
    "        n_samples, n_features = X_poly.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X_poly, self.weights) + self.bias\n",
    "\n",
    "            dw = (1/n_samples) * np.dot(X_poly.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights = self.weights - self.lr * dw\n",
    "            self.bias = self.bias - self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add polynomial features\n",
    "        X_poly = self._polynomial_features(X, self.degree)\n",
    "\n",
    "        # Normalize data\n",
    "        X_poly = self.scaler.transform(X_poly)\n",
    "\n",
    "        y_pred = np.dot(X_poly, self.weights) + self.bias\n",
    "        return y_pred\n",
    "\n",
    "    def _polynomial_features(self, X, degree):\n",
    "        n_samples, n_features = X.shape\n",
    "        X_poly = np.ones((n_samples, 1))\n",
    "\n",
    "        for d in range(1, degree+1):\n",
    "            for i in range(n_features):\n",
    "                X_poly = np.concatenate((X_poly, (X[:,i]**d).reshape(-1,1)), axis=1)\n",
    "\n",
    "        return X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1806a4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nagendra Swamy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#d = datasets.load_boston()\n",
    "#x, y = d.data, d.target\n",
    "\n",
    "# Load Boston dataset\n",
    "boston = datasets.load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1139c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e3e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and make predictions\n",
    "reg = PolynomialRegression(degree=2, lr=0.01, n_iters=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "predictions = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e45e1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.7906880716392194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "r2score = r2_score(y_test, predictions)\n",
    "print('R2 score:', r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9b1ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: (0.001, 1000)\n",
      "Validation r2score: 0.7906880716392194\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# define hyperparameters to tune\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "n_iters = [1000, 5000, 10000]\n",
    "\n",
    "# create all possible combinations of hyperparameters\n",
    "hyperparameters = list(product(lr, n_iters))\n",
    "\n",
    "# initialize best accuracy and corresponding hyperparameters\n",
    "best_r2score = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "# loop over all hyperparameters\n",
    "for hyperparameter in hyperparameters:\n",
    "    # create a new instance of LinearRegression with the current hyperparameters\n",
    "    lr = PolynomialRegression(lr=hyperparameter[0], n_iters=hyperparameter[1])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1234)\n",
    "    \n",
    "    # fit the model\n",
    "    lr.fit(X_train, y_train)\n",
    " \n",
    "    \n",
    "    # evaluate the model on the validation set\n",
    "    y_pred = lr.predict(X_test)\n",
    "    r2score=r2_score(y_test, predictions)\n",
    "    \n",
    "    # update best accuracy and corresponding hyperparameters\n",
    "    if r2score > best_r2score:\n",
    "        best_r2score= r2score\n",
    "        best_hyperparameters = hyperparameter\n",
    "        \n",
    "# print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Validation r2score:\", best_r2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3bd7f",
   "metadata": {},
   "source": [
    "# Model using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a94b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaff89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786    1  273   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875    1  273   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675    1  273   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889    1  273   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030   NaN  2.5050    1  273   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90    NaN  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99    NaN  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/Nagendra Swamy/Desktop/6000/HousingData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3f8d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       20\n",
       "ZN         20\n",
       "INDUS      20\n",
       "CHAS       20\n",
       "NOX         0\n",
       "RM          0\n",
       "AGE        20\n",
       "DIS         0\n",
       "RAD         0\n",
       "TAX         0\n",
       "PTRATIO     0\n",
       "B           0\n",
       "LSTAT      20\n",
       "MEDV        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a423b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>11.43</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>11.43</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>76.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786    1  273   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875    1  273   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675    1  273   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889    1  273   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  76.8  2.5050    1  273   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90  11.43  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99  11.43  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(df.median())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190f6d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa9cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 4)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(np.c_[df['LSTAT'], df['RM'], df['CRIM'], df['NOX']], columns=['LSTAT', 'RM', 'CRIM', 'NOX'])\n",
    "Y = df['MEDV']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0018dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 4)\n",
      "(127, 4)\n",
      "(379,)\n",
      "(127,)\n"
     ]
    }
   ],
   "source": [
    "# splits the training and test data set in 75% : 25%\n",
    "# assign random_state to any value.This ensures consistency.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=5)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2053bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression coefficients: [-0.56649976  5.35239531 -0.13861706 -0.60246748]\n",
      "Linear Regression intercept: -3.1562012904884504\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "\n",
    "print('Linear Regression coefficients: {}'.format(lm.coef_))\n",
    "print('Linear Regression intercept: {}'.format(lm.intercept_))\n",
    "\n",
    "# model evaluation for training set\n",
    "y_train_predict = lm.predict(X_train)\n",
    "\n",
    "# plt.plot(np.unique(Y_train), np.poly1d(np.polyfit(Y_train, y_train_predict, 1))(np.unique(Y_train)), \n",
    "#         linewidth=2, color='r')\n",
    "\n",
    "# calculating the intercept and slope for the regression line\n",
    "b, m = np.polynomial.polynomial.polyfit(Y_train, y_train_predict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca14780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The linear model performance for training set\n",
      "RMSE is 5.542915517308062\n",
      "R2 score is 0.6384156083074264\n"
     ]
    }
   ],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\n",
    "r2 = r2_score(Y_train, y_train_predict)\n",
    " \n",
    "print(\"The linear model performance for training set\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22203e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation for testing set\n",
    "y_test_predict = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880da3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The linear model performance for testing set\n",
      "RMSE is 5.5299665348068725\n",
      "R2 score is 0.6295124547192935\n"
     ]
    }
   ],
   "source": [
    "# root mean square error of the model\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    " \n",
    "# r-squared score of the model\n",
    "r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "print(\"\\nThe linear model performance for testing set\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34760e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Creates a polynomial regression model for the given degree\"\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "   \n",
    "# transform the features to higher degree features.\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "   \n",
    "# fit the transformed features to Linear Regression\n",
    "poly_model = LinearRegression()\n",
    "\n",
    "poly_model.fit(X_train_poly, Y_train)\n",
    "     \n",
    "# predicting on training data-set\n",
    "y_train_predicted = poly_model.predict(X_train_poly)\n",
    "   \n",
    "# predicting on test data-set\n",
    "y_test_predicted = poly_model.predict(poly_features.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cb49985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The polynomial model performance for the training set\n",
      "RMSE of training set is 4.24827458820609\n",
      "R2 score of training set is 0.7875981397531204\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model on training data-set\n",
    "rmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predicted))\n",
    "r2_train = r2_score(Y_train, y_train_predicted)\n",
    "     \n",
    "print(\"The polynomial model performance for the training set\")\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fcb65b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The polynomial model performance for the test set\n",
      "RMSE of test set is 4.166238351476593\n",
      "R2 score of test set is 0.7897108665470502\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model on test data-set\n",
    "rmse_test = np.sqrt(mean_squared_error(Y_test, y_test_predicted))\n",
    "r2_test = r2_score(Y_test, y_test_predicted)\n",
    "\n",
    "print(\"The polynomial model performance for the test set\")\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b026c",
   "metadata": {},
   "source": [
    "# Comparitive graph for accuracy of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17a0809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the accuracy scores for two different models\n",
    "model_names = ['model', 'model1']\n",
    "accuracy_scores = [0.7906880716392194, 0.7897108665470502]\n",
    "\n",
    "# Create a bar chart of the accuracy scores\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(model_names, accuracy_scores)\n",
    "\n",
    "# Set the chart title and labels\n",
    "ax.set_title('Model Accuracy Comparison')\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd5417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
